{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "817d228a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "from typing import List, Optional, Tuple, Union\n",
    "from transformers.cache_utils import Cache\n",
    "from transformers.activations import ACT2FN\n",
    "\n",
    "from transformers.modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    DynamicCache,\n",
    "    LogitsProcessorList,\n",
    "    TemperatureLogitsWarper,\n",
    "    TopKLogitsWarper,\n",
    "    TopPLogitsWarper,\n",
    ")\n",
    "from torch import nn\n",
    "import torch\n",
    "import os\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "\n",
    "class Qwen3Config:\n",
    "    # infra\n",
    "    HF_REPO_ID = \"yellooot/inferd-qwen3\"\n",
    "    # generation\n",
    "    TEMPERATURE: float = 0.6\n",
    "    TOP_K: int = 20\n",
    "    TOP_P: float = 0.95\n",
    "    EOS_TOKEN_ID: int = 151645\n",
    "    # model\n",
    "    HIDDEN_SIZE: int = 1024\n",
    "    NUM_HIDDEN_LAYERS: int = 28\n",
    "    NUM_ATTENTION_HEADS: int = 16\n",
    "    VOCAB_SIZE: int = 151936\n",
    "    MAX_POSITION_EMBEDDINGS: int = 40960\n",
    "    HEAD_DIM: int = 128\n",
    "    INTERMEDIATE_SIZE: int = 3072\n",
    "    PAD_TOKEN_ID: int = 151643\n",
    "    BOS_TOKEN_ID: int = 151643\n",
    "    RMS_NORM_EPS: float = 1e-6\n",
    "    HIDDEN_ACT: str = \"silu\"\n",
    "    NUM_KEY_VALUE_HEADS: int = 8\n",
    "    ATTENTION_BIAS: bool = False\n",
    "    ATTENTION_TYPE: str = \"full_attention\"\n",
    "    ROPE_THETA: int = 1000000\n",
    "    TORCH_DTYPE: str = \"bfloat16\"\n",
    "\n",
    "\n",
    "class Qwen3RMSNorm(nn.Module):\n",
    "    def __init__(self, hidden_size, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(hidden_size))\n",
    "        self.variance_epsilon = eps\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        input_dtype = hidden_states.dtype\n",
    "        hidden_states = hidden_states.to(torch.float32)\n",
    "        variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
    "        hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
    "        return self.weight * hidden_states.to(input_dtype)\n",
    "\n",
    "\n",
    "class Qwen3MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.hidden_size = Qwen3Config.HIDDEN_SIZE\n",
    "        self.intermediate_size = Qwen3Config.INTERMEDIATE_SIZE\n",
    "        self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n",
    "        self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n",
    "        self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size, bias=False)\n",
    "        self.act_fn = ACT2FN[Qwen3Config.HIDDEN_ACT]\n",
    "\n",
    "    def forward(self, x):\n",
    "        down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
    "        return down_proj\n",
    "\n",
    "\n",
    "def rotate_half(x):\n",
    "    x1 = x[..., : x.shape[-1] // 2]\n",
    "    x2 = x[..., x.shape[-1] // 2 :]\n",
    "    return torch.cat((-x2, x1), dim=-1)\n",
    "\n",
    "\n",
    "def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n",
    "    cos = cos.unsqueeze(unsqueeze_dim)\n",
    "    sin = sin.unsqueeze(unsqueeze_dim)\n",
    "    q_embed = (q * cos) + (rotate_half(q) * sin)\n",
    "    k_embed = (k * cos) + (rotate_half(k) * sin)\n",
    "    return q_embed, k_embed\n",
    "\n",
    "\n",
    "def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n",
    "    batch, num_key_value_heads, slen, head_dim = hidden_states.shape\n",
    "    if n_rep == 1:\n",
    "        return hidden_states\n",
    "    hidden_states = hidden_states[:, :, None, :, :].expand(\n",
    "        batch, num_key_value_heads, n_rep, slen, head_dim\n",
    "    )\n",
    "    return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)\n",
    "\n",
    "\n",
    "def eager_attention_forward(\n",
    "    module: nn.Module,\n",
    "    query: torch.Tensor,\n",
    "    key: torch.Tensor,\n",
    "    value: torch.Tensor,\n",
    "    attention_mask: Optional[torch.Tensor],\n",
    "    scaling: float,\n",
    "    dropout: float = 0.0,\n",
    "):\n",
    "    key_states = repeat_kv(key, module.num_key_value_groups)\n",
    "    value_states = repeat_kv(value, module.num_key_value_groups)\n",
    "\n",
    "    attn_weights = torch.matmul(query, key_states.transpose(2, 3)) * scaling\n",
    "    if attention_mask is not None:\n",
    "        causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]\n",
    "        attn_weights = attn_weights + causal_mask\n",
    "\n",
    "    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)\n",
    "    attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n",
    "    attn_output = torch.matmul(attn_weights, value_states)\n",
    "    attn_output = attn_output.transpose(1, 2).contiguous()\n",
    "\n",
    "    return attn_output, attn_weights\n",
    "\n",
    "\n",
    "class Qwen3Attention(nn.Module):\n",
    "    def __init__(self, layer_idx: int):\n",
    "        super().__init__()\n",
    "        self.layer_idx = layer_idx\n",
    "        self.head_dim = Qwen3Config.HEAD_DIM\n",
    "        self.num_key_value_groups = (\n",
    "            Qwen3Config.NUM_ATTENTION_HEADS // Qwen3Config.NUM_KEY_VALUE_HEADS\n",
    "        )\n",
    "        self.scaling = self.head_dim**-0.5\n",
    "        self.is_causal = True\n",
    "\n",
    "        self.q_proj = nn.Linear(\n",
    "            Qwen3Config.HIDDEN_SIZE,\n",
    "            Qwen3Config.NUM_ATTENTION_HEADS * self.head_dim,\n",
    "            bias=Qwen3Config.ATTENTION_BIAS,\n",
    "        )\n",
    "        self.k_proj = nn.Linear(\n",
    "            Qwen3Config.HIDDEN_SIZE,\n",
    "            Qwen3Config.NUM_KEY_VALUE_HEADS * self.head_dim,\n",
    "            bias=Qwen3Config.ATTENTION_BIAS,\n",
    "        )\n",
    "        self.v_proj = nn.Linear(\n",
    "            Qwen3Config.HIDDEN_SIZE,\n",
    "            Qwen3Config.NUM_KEY_VALUE_HEADS * self.head_dim,\n",
    "            bias=Qwen3Config.ATTENTION_BIAS,\n",
    "        )\n",
    "        self.o_proj = nn.Linear(\n",
    "            Qwen3Config.NUM_ATTENTION_HEADS * self.head_dim,\n",
    "            Qwen3Config.HIDDEN_SIZE,\n",
    "            bias=Qwen3Config.ATTENTION_BIAS,\n",
    "        )\n",
    "        self.q_norm = Qwen3RMSNorm(self.head_dim, eps=Qwen3Config.RMS_NORM_EPS)\n",
    "        self.k_norm = Qwen3RMSNorm(self.head_dim, eps=Qwen3Config.RMS_NORM_EPS)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: torch.Tensor,\n",
    "        position_embeddings: Tuple[torch.Tensor, torch.Tensor],\n",
    "        attention_mask: Optional[torch.Tensor],\n",
    "        past_key_value: Optional[Cache] = None,\n",
    "        cache_position: Optional[torch.LongTensor] = None,\n",
    "    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n",
    "        input_shape = hidden_states.shape[:-1]\n",
    "        hidden_shape = (*input_shape, -1, self.head_dim)\n",
    "\n",
    "        query_states = self.q_norm(self.q_proj(hidden_states).view(hidden_shape)).transpose(1, 2)\n",
    "        key_states = self.k_norm(self.k_proj(hidden_states).view(hidden_shape)).transpose(1, 2)\n",
    "        value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n",
    "\n",
    "        cos, sin = position_embeddings\n",
    "        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n",
    "\n",
    "        if past_key_value is not None:\n",
    "            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n",
    "            key_states, value_states = past_key_value.update(\n",
    "                key_states, value_states, self.layer_idx, cache_kwargs\n",
    "            )\n",
    "\n",
    "        attn_output, attn_weights = eager_attention_forward(\n",
    "            self,\n",
    "            query_states,\n",
    "            key_states,\n",
    "            value_states,\n",
    "            attention_mask,\n",
    "            dropout=0.0,\n",
    "            scaling=self.scaling,\n",
    "        )\n",
    "\n",
    "        attn_output = attn_output.reshape(*input_shape, -1).contiguous()\n",
    "        attn_output = self.o_proj(attn_output)\n",
    "        return attn_output, attn_weights\n",
    "\n",
    "\n",
    "class Qwen3DecoderLayer(nn.Module):\n",
    "    def __init__(self, layer_idx: int):\n",
    "        super().__init__()\n",
    "        self.hidden_size = Qwen3Config.HIDDEN_SIZE\n",
    "\n",
    "        self.self_attn = Qwen3Attention(layer_idx)\n",
    "\n",
    "        self.mlp = Qwen3MLP()\n",
    "        self.input_layernorm = Qwen3RMSNorm(Qwen3Config.HIDDEN_SIZE, eps=Qwen3Config.RMS_NORM_EPS)\n",
    "        self.post_attention_layernorm = Qwen3RMSNorm(\n",
    "            Qwen3Config.HIDDEN_SIZE, eps=Qwen3Config.RMS_NORM_EPS\n",
    "        )\n",
    "        self.attention_type = Qwen3Config.ATTENTION_TYPE\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: torch.Tensor,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        past_key_value: Optional[Cache] = None,\n",
    "        cache_position: Optional[torch.LongTensor] = None,\n",
    "        position_embeddings: Optional[\n",
    "            Tuple[torch.Tensor, torch.Tensor]\n",
    "        ] = None,  # necessary, but kept here for BC\n",
    "    ) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:\n",
    "        residual = hidden_states\n",
    "        hidden_states = self.input_layernorm(hidden_states)\n",
    "\n",
    "        hidden_states, _ = self.self_attn(\n",
    "            hidden_states=hidden_states,\n",
    "            position_embeddings=position_embeddings,\n",
    "            attention_mask=attention_mask,\n",
    "            past_key_value=past_key_value,\n",
    "            cache_position=cache_position,\n",
    "        )\n",
    "        hidden_states = residual + hidden_states\n",
    "\n",
    "        residual = hidden_states\n",
    "        hidden_states = self.post_attention_layernorm(hidden_states)\n",
    "        hidden_states = self.mlp(hidden_states)\n",
    "        hidden_states = residual + hidden_states\n",
    "\n",
    "        return hidden_states\n",
    "\n",
    "\n",
    "class Qwen3RotaryEmbedding(nn.Module):\n",
    "    def __init__(self, device=None):\n",
    "        super().__init__()\n",
    "        self.rope_type = \"default\"\n",
    "        self.max_seq_len_cached = Qwen3Config.MAX_POSITION_EMBEDDINGS\n",
    "        self.original_max_seq_len = Qwen3Config.MAX_POSITION_EMBEDDINGS\n",
    "\n",
    "        self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n",
    "\n",
    "        inv_freq, self.attention_scaling = self.rope_init_fn(\n",
    "            device=device, base=Qwen3Config.ROPE_THETA, dim=Qwen3Config.HEAD_DIM\n",
    "        )\n",
    "        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n",
    "        self.original_inv_freq = self.inv_freq\n",
    "\n",
    "    @torch.no_grad()\n",
    "    @dynamic_rope_update\n",
    "    def forward(self, x, position_ids):\n",
    "        inv_freq_expanded = (\n",
    "            self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1).to(x.device)\n",
    "        )\n",
    "        position_ids_expanded = position_ids[:, None, :].float()\n",
    "\n",
    "        device_type = (\n",
    "            x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n",
    "        )\n",
    "        with torch.autocast(device_type=device_type, enabled=False):\n",
    "            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n",
    "            emb = torch.cat((freqs, freqs), dim=-1)\n",
    "            cos = emb.cos() * self.attention_scaling\n",
    "            sin = emb.sin() * self.attention_scaling\n",
    "\n",
    "        return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n",
    "\n",
    "\n",
    "class Qwen3Server(nn.Module):\n",
    "    def __init__(self, start_layer: int, end_layer: int):\n",
    "        super().__init__()\n",
    "        if torch.cuda.is_available():\n",
    "            self.device = torch.device(\"cuda\")\n",
    "        elif torch.backends.mps.is_available():\n",
    "            self.device = torch.device(\"mps\")\n",
    "        else:\n",
    "            self.device = torch.device(\"cpu\")\n",
    "        self.start_layer = start_layer\n",
    "        self.end_layer = end_layer\n",
    "        self.past_key_value = DynamicCache()\n",
    "        self.local_layers = nn.ModuleList(\n",
    "            [Qwen3DecoderLayer(layer_idx) for layer_idx in range(start_layer, end_layer + 1)]\n",
    "        )\n",
    "        self.to(self.device)\n",
    "        self._load_weights()\n",
    "\n",
    "    def _load_weights(self):\n",
    "        for layer in self.local_layers:\n",
    "            idx = layer.self_attn.layer_idx\n",
    "            fname = f\"layer_{idx:02d}.pt\"\n",
    "\n",
    "            # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ñ–∞–π–ª —Å HF (–∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –∫—ç—à–∏—Ä—É–µ—Ç—Å—è –≤ ~/.cache/huggingface)\n",
    "            cached_path = hf_hub_download(repo_id=Qwen3Config.HF_REPO_ID, filename=fname)\n",
    "\n",
    "            layer_sd = torch.load(cached_path, map_location=self.device)\n",
    "            layer.load_state_dict(layer_sd)\n",
    "\n",
    "    def send(\n",
    "        self,\n",
    "        hidden_states,\n",
    "        attention_mask,\n",
    "        cache_position,\n",
    "        position_embeddings,\n",
    "        input_start_layer=None,\n",
    "        input_end_layer=None,\n",
    "    ):\n",
    "        for layer in self.local_layers:\n",
    "            hidden_states = layer(\n",
    "                hidden_states=hidden_states,\n",
    "                attention_mask=attention_mask,\n",
    "                cache_position=cache_position,\n",
    "                position_embeddings=position_embeddings,\n",
    "                past_key_value=self.past_key_value,\n",
    "            )\n",
    "        return hidden_states\n",
    "\n",
    "\n",
    "class Qwen3Client(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        if torch.cuda.is_available():\n",
    "            self.device = torch.device(\"cuda\")\n",
    "        elif torch.backends.mps.is_available():\n",
    "            self.device = torch.device(\"mps\")\n",
    "        else:\n",
    "            self.device = torch.device(\"cpu\")\n",
    "\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen3-0.6B\")\n",
    "\n",
    "        self.embed_tokens = nn.Embedding(\n",
    "            Qwen3Config.VOCAB_SIZE, Qwen3Config.HIDDEN_SIZE, Qwen3Config.PAD_TOKEN_ID\n",
    "        )\n",
    "        self.norm = Qwen3RMSNorm(Qwen3Config.HIDDEN_SIZE, Qwen3Config.RMS_NORM_EPS)\n",
    "        self.lm_head = nn.Linear(Qwen3Config.HIDDEN_SIZE, Qwen3Config.VOCAB_SIZE, bias=False)\n",
    "        self.rotary_emb = Qwen3RotaryEmbedding()\n",
    "        self.to(self.device)\n",
    "        self._load_weights()\n",
    "\n",
    "        self.logit_processors = LogitsProcessorList(\n",
    "            [\n",
    "                TemperatureLogitsWarper(Qwen3Config.TEMPERATURE),\n",
    "                TopKLogitsWarper(Qwen3Config.TOP_K),\n",
    "                TopPLogitsWarper(Qwen3Config.TOP_P),\n",
    "            ]\n",
    "        )\n",
    "        self._dummy_input_ids = torch.zeros((1, 1), dtype=torch.long, device=self.device)\n",
    "\n",
    "    def _load_weights(self):\n",
    "        for fname, target_module in [\n",
    "            (\"embed_tokens.pt\", self.embed_tokens),\n",
    "            (\"norm.pt\", self.norm),\n",
    "            (\"lm_head.pt\", self.lm_head),\n",
    "        ]:\n",
    "            file_path = hf_hub_download(repo_id=Qwen3Config.HF_REPO_ID, filename=fname)\n",
    "            state_dict = torch.load(file_path, map_location=self.device)\n",
    "            target_module.load_state_dict(state_dict)\n",
    "\n",
    "    def _choose_next(self, logits: torch.Tensor) -> torch.Tensor:\n",
    "        logits = logits.clone()\n",
    "        logits = self.logit_processors(self._dummy_input_ids, logits)\n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "        next_token_id = torch.multinomial(probs, num_samples=1)\n",
    "        return next_token_id\n",
    "\n",
    "    def _should_continue(self, generated_ids: List[int], max_length: Optional[int]) -> bool:\n",
    "        if max_length is not None and len(generated_ids) >= max_length:\n",
    "            return False\n",
    "\n",
    "        if len(generated_ids) > 0 and generated_ids[-1] == Qwen3Config.EOS_TOKEN_ID:\n",
    "            return False\n",
    "\n",
    "        return True\n",
    "\n",
    "    def _find_best_chain(self, known_servers):\n",
    "        return deque(\n",
    "            [\n",
    "                Qwen3Server(0, 9),\n",
    "                Qwen3Server(10, 19),\n",
    "                Qwen3Server(20, Qwen3Config.NUM_HIDDEN_LAYERS - 1),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def _update_causal_mask(\n",
    "        self,\n",
    "        attention_mask: torch.Tensor,\n",
    "        input_tensor: torch.Tensor,\n",
    "        cache_position: torch.Tensor,\n",
    "        past_seen_tokens: int,\n",
    "    ):\n",
    "        dtype = input_tensor.dtype\n",
    "        sequence_length = input_tensor.shape[1]\n",
    "        target_length = (\n",
    "            attention_mask.shape[-1]\n",
    "            if isinstance(attention_mask, torch.Tensor)\n",
    "            else past_seen_tokens + sequence_length + 1\n",
    "        )\n",
    "        causal_mask = self._prepare_4d_causal_attention_mask_with_cache_position(\n",
    "            attention_mask,\n",
    "            sequence_length=sequence_length,\n",
    "            target_length=target_length,\n",
    "            dtype=dtype,\n",
    "            cache_position=cache_position,\n",
    "            batch_size=input_tensor.shape[0],\n",
    "        )\n",
    "\n",
    "        return causal_mask\n",
    "\n",
    "    @staticmethod\n",
    "    def _prepare_4d_causal_attention_mask_with_cache_position(\n",
    "        attention_mask: torch.Tensor,\n",
    "        sequence_length: int,\n",
    "        target_length: int,\n",
    "        dtype: torch.dtype,\n",
    "        cache_position: torch.Tensor,\n",
    "        batch_size: int,\n",
    "    ):\n",
    "        if attention_mask is not None and attention_mask.dim() == 4:\n",
    "            causal_mask = attention_mask\n",
    "        else:\n",
    "            min_dtype = torch.finfo(dtype).min\n",
    "            causal_mask = torch.full(\n",
    "                (sequence_length, target_length),\n",
    "                fill_value=min_dtype,\n",
    "                dtype=dtype,\n",
    "                device=cache_position.device,\n",
    "            )\n",
    "            diagonal_attend_mask = torch.arange(\n",
    "                target_length, device=cache_position.device\n",
    "            ) > cache_position.reshape(-1, 1)\n",
    "\n",
    "            causal_mask *= diagonal_attend_mask\n",
    "            causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n",
    "            if attention_mask is not None:\n",
    "                causal_mask = causal_mask.clone()\n",
    "                if attention_mask.shape[-1] > target_length:\n",
    "                    attention_mask = attention_mask[:, :target_length]\n",
    "                mask_length = attention_mask.shape[-1]\n",
    "                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[\n",
    "                    :, None, None, :\n",
    "                ].to(causal_mask.device)\n",
    "                padding_mask = padding_mask == 0\n",
    "                causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n",
    "                    padding_mask, min_dtype\n",
    "                )\n",
    "        return causal_mask\n",
    "\n",
    "    def generate(self, known_servers, prompt: str, max_length: Optional[int] = None) -> str:\n",
    "        chain = self._find_best_chain(known_servers)\n",
    "        with torch.no_grad():\n",
    "            generated_ids: List[int] = []\n",
    "            messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "            text = self.tokenizer.apply_chat_template(\n",
    "                messages,\n",
    "                tokenize=False,\n",
    "                add_generation_prompt=True,\n",
    "                enable_thinking=False,\n",
    "            )\n",
    "            all_input_ids = self.tokenizer(text, return_tensors=\"pt\").input_ids.to(self.device)\n",
    "\n",
    "            input_embeds = self.embed_tokens(all_input_ids)\n",
    "            batch_size, seq_len = input_embeds.shape[0], input_embeds.shape[1]\n",
    "\n",
    "            position_ids = torch.arange(seq_len, device=self.device).unsqueeze(0)\n",
    "\n",
    "            dtype = input_embeds.dtype\n",
    "            min_val = torch.finfo(dtype).min\n",
    "            tril_2d = torch.tril(torch.ones(seq_len, seq_len, device=self.device, dtype=dtype))\n",
    "            mask2d = (1.0 - tril_2d) * min_val\n",
    "            causal_mask_4d = (\n",
    "                mask2d.unsqueeze(0).unsqueeze(0).expand(batch_size, 1, seq_len, seq_len)\n",
    "            )\n",
    "\n",
    "            pos_emb = self.rotary_emb(input_embeds, position_ids)\n",
    "            hidden_states = input_embeds\n",
    "\n",
    "            for server in chain:\n",
    "                hidden_states = server.send(\n",
    "                    hidden_states=hidden_states,\n",
    "                    attention_mask=causal_mask_4d,\n",
    "                    cache_position=position_ids.squeeze(0),\n",
    "                    position_embeddings=pos_emb,\n",
    "                )\n",
    "\n",
    "            hidden_last = hidden_states[:, -1, :]\n",
    "            hidden_norm = self.norm(hidden_last)\n",
    "            last_logits = self.lm_head(hidden_norm)\n",
    "\n",
    "            next_token_id = self._choose_next(last_logits)\n",
    "            generated_ids.append(next_token_id.item())\n",
    "\n",
    "            while self._should_continue(generated_ids, max_length):\n",
    "                single_input_ids = next_token_id\n",
    "                emb = self.embed_tokens(single_input_ids)\n",
    "\n",
    "                small_mask2d = torch.zeros((1, 1), device=self.device, dtype=dtype)\n",
    "                causal_mask_single = small_mask2d.unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "                past_len = all_input_ids.shape[1] + len(generated_ids) - 1\n",
    "\n",
    "                position_ids = torch.tensor([[past_len]], device=self.device)\n",
    "\n",
    "                pos_emb = self.rotary_emb(emb, position_ids)\n",
    "\n",
    "                hidden_states = emb\n",
    "                for server in chain:\n",
    "                    hidden_states = server.send(\n",
    "                        hidden_states=hidden_states,\n",
    "                        attention_mask=causal_mask_single,\n",
    "                        cache_position=position_ids.squeeze(0),\n",
    "                        position_embeddings=pos_emb,\n",
    "                    )\n",
    "\n",
    "                hidden_last = hidden_states[:, -1, :]\n",
    "                hidden_norm = self.norm(hidden_last)\n",
    "                last_logits = self.lm_head(hidden_norm)\n",
    "\n",
    "                next_token_id = self._choose_next(last_logits)\n",
    "                generated_ids.append(next_token_id.item())\n",
    "\n",
    "                print(\n",
    "                    self.tokenizer.decode(\n",
    "                        [next_token_id.item()],\n",
    "                        skip_special_tokens=True,\n",
    "                        clean_up_tokenization_spaces=True,\n",
    "                    ),\n",
    "                    end=\"\",\n",
    "                    flush=True,\n",
    "                )\n",
    "\n",
    "            output_str = self.tokenizer.decode(\n",
    "                generated_ids, skip_special_tokens=True, clean_up_tokenization_spaces=True\n",
    "            )\n",
    "            return output_str\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0aea1a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Qwen3Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2da358b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " love cats too! They're such amazing pets. ÔøΩÔøΩ They can be super playful, curious, and loving. If you're looking for a cat, I'd love to help you find one! ÔøΩÔøΩÔøΩ"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"I love cats too! They're such amazing pets. üòä They can be super playful, curious, and loving. If you're looking for a cat, I'd love to help you find one! üêæ\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.generate(None, \"I love cats!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7818cfd2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
